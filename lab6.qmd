---
title: "lab6"
format: 
  html:
    self-contained: true
---
```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
```

```{r}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'

download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')

walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 

camels <- power_full_join(camels ,by = 'gauge_id')

# Read and merge data
camels <- map(remote_files, read_delim, show_col_types = FALSE) |> 
  power_full_join(by = 'gauge_id')
```
#1
```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
#zero_q_freq means frequency of days where Q = 0 mm/day
```

#2
```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = aridity)) +
  scale_color_gradient(low = "darkgreen", high = "brown") +
  ggthemes::theme_map()

ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = p_mean)) +
  scale_color_gradient(low = "gray", high = "dodgerblue") +
  ggthemes::theme_map()
```

```{r}
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages("xgboost")

```


#3
```{r}
set.seed(123)
# Bad form to perform simple transformations on the outcome variable within a 
# recipe. So, we'll do it here.
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)

rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())

### --- END FROM LAB --- ###
install.packages("xgboost")
library(xgboost)
xgb <- boost_tree(mode = "regression", trees = 1000)

nn <- bag_mlp(mode = "regression") %>%
  set_engine("nnet")

xgb_workflow <- workflow() %>%
  add_model(xgb) %>% 
  add_recipe(rec) %>%
  fit(data = camels_train) |> 
  augment(camels_train)

nn_workflow <- workflow() %>%
  add_model(nn) %>%
  add_recipe(rec) %>%
  fit(data = camels_train)

```
#4
```{r}
## **Q4a**: Data Prep / Data Splitting (15)
camels2 <- camels %>%
  mutate(logQmean = log(q_mean)) %>%
  select(logQmean, p_mean, aridity, soil_depth_pelletier, max_water_content, organic_frac, frac_snow, pet_mean, soil_depth_statsgo, elev_mean) %>%
  na.omit()

set.seed(1991)
c_split <-  initial_split(camels2, prop = 0.8)
c_train <-  training(c_split)
c_test  <-  testing(c_split)
cv <-  vfold_cv(c_train, v = 10)

## **Q4b**: Recipe (15)
rec <-  recipe(logQmean ~ ., data = c_train) %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors())

## **Q4c**: Define 3 models (25)
dt <- decision_tree(mode = "regression") %>%
  set_engine("rpart")

xgb <- boost_tree(mode = "regression", trees = 1000)

rf <- rand_forest(mode = "regression") %>%
  set_engine("ranger")

nn <- bag_mlp(mode = "regression") %>%
  set_engine("nnet")

# **Q4d**: workflow set (15)
wf_obj <- workflow_set(list(rec), list(rf, nn, xgb, dt)) %>%
  workflow_map("fit_resamples", 
               resamples = cv,
               control = control_resamples(save_pred = TRUE)) 

# **Q4e**: Evaluation (15)
autoplot(wf_obj)
rank_results(wf_obj, rank_metric = "rmse")

# **Q4f**: Extract and Evaluate (15)

m <- workflow() %>%
  add_model(nn) %>%
  add_recipe(rec) %>%
  fit(data = c_train)

a <- augment(m, c_test) 

ggplot(a, aes(x = logQmean, y = .pred)) +
  geom_point() +
  geom_abline() +
  labs(title = "Neural Network Model", x = "Truth", y = "Prediction") +
  theme_minimal()
  
metrics(a, truth = logQmean, estimate = .pred) 

```

```{r}
url("https://cloud.r-project.org")

```

